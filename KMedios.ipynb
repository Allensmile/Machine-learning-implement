{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一篇文章我们详细讲解了 k-means 算法，这篇文章简单介绍一下 k-medoids 算法，从名字上就可以看出来，这两个算法应该有些相似的地方。其实，k-medoids 可以算是 k-means 的一个变种。\n",
    "\n",
    "k-medoids 和 k-means 不一样的地方在于中心点的选取，在 k-means 中，我们将中心点取为当前 cluster 中所有数据点的平均值。然而在 k-medoids 中，我们将中心点的选取限制在当前 cluster 所包含的数据点的集合中。换句话说，在 k-medoids 算法中，我们将从当前 cluster 中选取这样一个点——它到其他所有(当前 cluster 中的)点的距离之和最小——作为中心点。k-means 和 k-medoids 之间的差异就类似于一个数据样本的均值 (mean) 和中位数 (median) 之间的差异：前者的取值范围可以是连续空间中的任意值，而后者只能在给样本给定的那些点里面选。那么，这样做的好处是什么呢？\n",
    "\n",
    "一个最直接的理由就是 k-means 对数据的要求太高了，它使用欧氏距离描述数据点之间的差异(dissimilarity)，从而可以直接通过求均值来计算中心点。这要求数据点处在一个欧氏空间之中。\n",
    "\n",
    "然而并不是所有的数据都能满足这样的要求，对于数值类型的特征，比如身高，可以很自然地用这样的方式来处理，但是类别 (categorical)类型的特征就不行了。举一个简单的例子，如果我现在要对犬进行聚类，并且希望直接在所有犬组成的空间中进行，k-means 就无能为力了，因为欧氏距离 $ \\|x_i-x_j\\|^2 $在这里不能用了：一只 Samoyed 减去一只 Rough Collie 然后在平方一下？天知道那是什么！再加上一只 German Shepherd Dog 然后求一下平均值？根本没法算，k-means 在这里寸步难行！\n",
    "\n",
    "在 k-medoids 中，我们把原来的目标函数 J 中的欧氏距离改为一个任意的 dissimilarity measure 函数 $\\mathcal{V}$：\n",
    "\n",
    "$$ \\displaystyle\\tilde{J} = \\sum_{n=1}^N\\sum_{k=1}^K r_{nk}\\mathcal{V}(x_n,\\mu_k) $$\n",
    "\n",
    "最常见的方式是构造一个dissimilarity matrix $ \\mathbf{D} 来代表 \\mathcal{V}$，其中的元素 $\\mathbf{D}_{ij}$ 表示第 i 只狗和第 j 只狗之间的差异程度，例如，两只 Samoyed 之间的差异可以设为 0 ，一只 German Shepherd Dog 和一只 Rough Collie 之间的差异是 0.7，和一只 Miniature Schnauzer 之间的差异是 1 ，等等。\n",
    "\n",
    "除此之外，由于中心点是在已有的数据点里面选取的，因此相对于 k-means 来说，不容易受到那些由于误差之类的原因产生的 Outlier 的影响，更加 robust 一些。\n",
    "\n",
    "扯了这么多，还是直接来看看 k-medoids 的效果好了，由于 k-medoids 对数据的要求比 k-means 要低，所以 k-means 能处理的情况自然 k-medoids 也能处理，为了能先睹为快，我们偷一下懒，直接在上一篇文章中的 k-means 代码的基础上稍作一点修改，还用同样的例子。将代码的 45 到 47 行改成下面这样："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "围绕中心点划分（Partitioning Around Medoids，PAM）的方法是比较常用的，使用PAM方法进行处理，可以指定一个最大迭代次数的参数，在迭代过程中基于贪心策略来选择使得聚类的质量最高的划分。使用PAM的方法处理，每次交换一个中心点和非中心点，然后执行将非中心点指派到最近的中心点，计算得到的SAD值越小，则聚类质量越好，如此不断地迭代，直到找到一个最好的划分。详细的算法流程如下：\n",
    "\n",
    "\n",
    "算法流程：\n",
    "\n",
    "输入：数据集X，簇的个数k，最大迭代次数max_iterations，迭代停止条件varepsilon\n",
    "\n",
    "输出：每个样本的预测标签\n",
    "\n",
    "    从待聚类的数据集X中随机选择k个样本点，作为初始中心点；\n",
    "\n",
    "    将待聚类的数据集X中的点，指派到最近的中心点，即进行簇的划分；\n",
    "\n",
    "    进行迭代，直到聚类的质量满足指定的阈值(可以通过计算SAD)，使总代价cost减少：\n",
    "\n",
    "        lowest_cost = inf\n",
    "        遍历所有中心(medoids)：\n",
    "            对每一个中心点medoid，对每一个非中心点sample，执行如下计算步骤：\n",
    "                将中心点medoids和sample进行交换；\n",
    "                根据新的中心点进行簇的划分；\n",
    "                重新计算该划分的cost；\n",
    "                若新的cost<lowest_cost，则将中心进行更新，同时令lowest_cost = cost，否则不交换\n",
    "    退出迭代\n",
    "\n",
    "\n",
    "\n",
    "如果仔细看上面算法流程的话，就会发现，从 k-means 变到 k-medoids ，时间复杂度陡然增加了许多：在 k-means 中只要求一个平均值即可，而在 k-medoids 中则需要遍历每个样本点，并求出它到所有其他点的距离之和。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "上面算法描述，应该是按顺序的取遍中心点集合中的点，也从非中心点集合中取遍所有非中心点，分别计算生成的新划分的代价。由于待聚类的点集可大可小，我们可以考虑，每次取点的时候，采用随机取点的策略，随机性越强越好，只要满足最终迭代终止的条件即可。通常，如果能够迭代所有情况，那么最终得到的划分一定是最优的划分，即聚类结果最好，这通常适用于聚类比较小的点的集合。但是如果待聚类的点的集合比较大，则需要通过限制迭代次数来终止迭代计算，从而得到一个能够满足实际精度需要的聚类结果。\n",
    "我们在下面实现k-medoids聚类算法，分别随机选择中心点和非中心点，对他们进行交换，通过设置允许最大迭代次数(max_iterations)这个参数值，来使聚类计算最后停止。\n",
    "\n",
    "\n",
    "## 聚类算法代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  2.  2.  1.  2.  0.  0.  2.  1.  0.  1.  2.  1.  1.  2.  1.  1.\n",
      "  2.  1.  2.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.  2.  1.  2.  1.  2.\n",
      "  0.  2.  2.  2.  2.  1.  2.  0.  0.  2.  0.  0.  1.  1.  0.  1.  0.  1.\n",
      "  1.  2.  2.  1.  1.  0.  0.  2.  0.  1.  2.  0.  1.  0.  0.  2.  0.  2.\n",
      "  0.  0.  2.  0.  0.  2.  0.  0.  1.  0.  1.  2.  1.  0.  1.  2.  1.  0.\n",
      "  2.  2.  1.  2.  1.  1.  2.  2.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 正规化数据集 X\n",
    "def normalize(X, axis=-1, p=2):\n",
    "    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))\n",
    "    lp_norm[lp_norm == 0] = 1\n",
    "    return X / np.expand_dims(lp_norm, axis)\n",
    "\n",
    "\n",
    "# 计算一个样本与数据集中所有样本的欧氏距离的平方\n",
    "def euclidean_distance(one_sample, X):\n",
    "    one_sample = one_sample.reshape(1, -1)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    distances = np.power(np.tile(one_sample, (X.shape[0], 1)) - X, 2).sum(axis=1)\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class KMedoids():\n",
    "    \"\"\"\n",
    "    k-medoids聚类算法.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k: int\n",
    "        聚类簇的数目.\n",
    "    max_iterations: int\n",
    "        最大迭代次数. \n",
    "    varepsilon: float\n",
    "        判断是否收敛, 如果上一次的所有k个聚类中心与本次的所有k个聚类中心的差都小于varepsilon, \n",
    "        则说明算法已经收敛\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, max_iterations=500, varepsilon=0.0001):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "        self.varepsilon = varepsilon\n",
    "\n",
    "    # 随机初始化k个聚类中心\n",
    "    def init_random_medoids(self, X):\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        medoids = np.zeros((self.k, n_features))\n",
    "        for i in range(self.k):\n",
    "            medoid = X[np.random.choice(range(n_samples))]\n",
    "            medoids[i] = medoid\n",
    "        return medoids\n",
    "\n",
    "    # 返回离该样本最近的中心的索引\n",
    "    def closest_medoid(self, sample, medoids):\n",
    "        distances = euclidean_distance(sample, medoids)\n",
    "        closest_i = np.argmin(distances)\n",
    "        return closest_i\n",
    "\n",
    "    # 将每一个样本分配到与其最近的一个中心\n",
    "    def create_clusters(self, X, medoids):\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "        for sample_i, sample in enumerate(X):\n",
    "            medoid_i = self.closest_medoid(sample, medoids)\n",
    "            clusters[medoid_i].append(sample_i)\n",
    "        return clusters\n",
    "\n",
    "    # 计算cost (所有样本到其相应中心的距离之和)\n",
    "    def calculate_cost(self, X, clusters, medoids):\n",
    "        cost = 0\n",
    "        # For each cluster\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            medoid = medoids[i]\n",
    "            cost += euclidean_distance(medoid, X[cluster]).sum()\n",
    "        return cost\n",
    "\n",
    "    # Returns a list of all samples that are not currently medoids\n",
    "    def get_X_no_medoids(self, X, medoids):\n",
    "        no_medoids = []\n",
    "        for sample in X:\n",
    "            if not sample in medoids:\n",
    "                no_medoids.append(sample)\n",
    "        return no_medoids\n",
    "\n",
    "    # 获取每个样本的label, 方法是将每个簇的索引号记做该簇中样本的label\n",
    "    def get_cluster_labels(self, clusters, X):\n",
    "        y_pred = np.zeros(np.shape(X)[0])\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            y_pred[cluster] = i\n",
    "        return y_pred\n",
    "\n",
    "    # Do Partitioning Around Medoids and return the cluster labels\n",
    "    def predict(self, X):\n",
    "        # 随机初始化self.k个中心\n",
    "        medoids = self.init_random_medoids(X)\n",
    "        # 进行cluster，将整个数据集中样本分配到与其最近的中心\n",
    "        clusters = self.create_clusters(X, medoids)\n",
    "\n",
    "        # 计算初始损失 (所有样本到其相应中心的距离之和)\n",
    "        cost = self.calculate_cost(X, clusters, medoids)\n",
    "\n",
    "        # 迭代, 直到 cost 不再下降\n",
    "        while True:\n",
    "            best_medoids = medoids\n",
    "            lowest_cost = cost\n",
    "            # 遍历所有中心(或者簇(clusters))\n",
    "            for medoid in medoids:\n",
    "                # 获取所有非中心的样本\n",
    "                X_no_medoids = self.get_X_no_medoids(X, medoids)\n",
    "                # 遍历所有非中心的样本\n",
    "                for sample in X_no_medoids:\n",
    "                    # Swap sample with the medoid\n",
    "                    new_medoids = medoids.copy()\n",
    "                    new_medoids[medoids == medoid] = sample\n",
    "                    # 按照新的中心划分簇(clusters)\n",
    "                    new_clusters = self.create_clusters(X, new_medoids)\n",
    "                    # 计算中心更新之后的 cost\n",
    "                    new_cost = self.calculate_cost(X, new_clusters, new_medoids)\n",
    "                    # 如果中心更新之后的cost < 更新之前的cost, 则将中心, cost进行更新\n",
    "                    if new_cost < lowest_cost:\n",
    "                        lowest_cost = new_cost\n",
    "                        best_medoids = new_medoids\n",
    "            # If there was a swap that resultet in a lower cost we save the\n",
    "            # resulting medoids from the best swap and the new cost \n",
    "            if lowest_cost < cost:\n",
    "                cost = lowest_cost\n",
    "                medoids = best_medoids\n",
    "            # Else finished\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # 按照最终(最优)的中心再划分簇(clusters)\n",
    "        final_clusters = self.create_clusters(X, medoids)\n",
    "        # 按照最终(最优)的簇(clusters)获取所有样本的label\n",
    "        return self.get_cluster_labels(final_clusters, X)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    X, y = datasets.make_blobs()\n",
    "\n",
    "    # Cluster the data using K-Medoids\n",
    "    clf = KMedoids(k=3)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
