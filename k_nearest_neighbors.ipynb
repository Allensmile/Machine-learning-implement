{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近邻分类算法 (K-Nearest Neighbor) \n",
    "\n",
    "\n",
    "KNN分类算法非常简单，该算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别。该方法在确定分类决策上只依据最邻近K个样本的类别来决定待分样本所属的类别。KNN是一个懒惰算法，也就是说在平时不好好学习，考试（对测试样本分类）时才临阵发力（临时去找k个近邻），因此在预测的时候速度比较慢。\n",
    "\n",
    "KNN模型是非参数模型，既然有非参数模型，那就肯定还有参数模型，那何为参数模型与非参数模型呢？\n",
    "\n",
    "\n",
    "\n",
    "## 1. 参数模型与非参数模型\n",
    "\n",
    "### 1.1 参数模型\n",
    "\n",
    "参数模型是指选择某种形式的函数并通过机器学习用一系列固定个数的参数尽可能表征这些数据的某种模式。参数模型具有如下特征：\n",
    "1. 不管数据量有多大，在模型确定了，参数的个数就确定了，即参数个数不随着样本量的增大而增加，从关系上说它们相互独立；\n",
    "2. 一般参数模型会对数据有一定的假设，如分布的假设，空间的假设等，并且这些假设可以由参数来描述；\n",
    "3. 参数模型预测速度快。\n",
    "\n",
    "常用参数学习的模型有： \n",
    "\n",
    "* 回归模型（线性回归、岭回归、lasso回归、多项式回归）\n",
    "* 逻辑回归\n",
    "* 线性判别分析（Linear Discriminant Analysis）\n",
    "* 感知器\n",
    "* 朴素贝叶斯\n",
    "* 神经网络\n",
    "* 使用线性核的SVM\n",
    "* Mixture models\n",
    "* K-means\n",
    "* Hidden Markov models\n",
    "* Factor analysis / pPCA / PMF\n",
    "\n",
    "\n",
    "### 1.2 非参数模型\n",
    "\n",
    "非参数模型是指系统的数学模型中非显式地包含可估参数。注意不要被名字误导，非参不等于无参。非参数模型具有以下特征：\n",
    "\n",
    "1. 数据决定了函数形式，函数参数个数不固定；\n",
    "2. 随着训练数据量的增加，参数个数一般也会随之增长，模型越来越大；\n",
    "3. 对数据本身做较少的先验假设；\n",
    "4. 预测速度慢。\n",
    "\n",
    "一些常用的非参学习模型： \n",
    "\n",
    "* k-Nearest Neighbors\n",
    "* Decision Trees like CART and C4.5\n",
    "* 使用非线性核的SVM\n",
    "* Gradient Boosted Decision Trees\n",
    "* Gaussian processes for regression\n",
    "* Dirichlet process mixtures\n",
    "* infinite HMMs\n",
    "* infinite latent factor models\n",
    "\n",
    "\n",
    "## 2. KNN算法步骤：\n",
    "\n",
    "1. 准备数据，对数据进行预处理；\n",
    "\n",
    "2. 设定参数，如k；\n",
    "\n",
    "3. 遍历测试集，\n",
    "\n",
    "      对测试集中每个样本，计算该样本（测试集中）到训练集中每个样本的距离；\n",
    "\n",
    "      取出训练集中到该样本（测试集中）的距离最小的k个样本的类别标签；\n",
    "\n",
    "      对类别标签进行计数，类别标签次数最多的就是该样本（测试集中）的类别标签。\n",
    "\n",
    "\n",
    "4. 遍历完毕.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. KNN算法优点和缺点\n",
    "\n",
    "### 3.1 KNN算法优点\n",
    "\n",
    "1. 简单，易于理解，易于实现，无需估计参数，无需训练；\n",
    "\n",
    "2. 适合对稀有事件进行分类；\n",
    "\n",
    "3. 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好；\n",
    "\n",
    "4. 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合；\n",
    "\n",
    "5. 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 KNN算法缺点\n",
    "\n",
    "1. 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进；\n",
    "\n",
    "2. 该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。\n",
    "\n",
    "3. 属于硬分类，即直接给出这个样本的类别，并不是给出这个样本有多大的可能性属于该类别；\n",
    "\n",
    "4. 可解释性差，无法给出像决策树那样的规则；\n",
    "\n",
    "5. 计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.924242424242\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def shuffle_data(X, y, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "\n",
    "# 正规化数据集 X\n",
    "def normalize(X, axis=-1, p=2):\n",
    "    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))\n",
    "    lp_norm[lp_norm == 0] = 1\n",
    "    return X / np.expand_dims(lp_norm, axis)\n",
    "\n",
    "\n",
    "# 标准化数据集 X\n",
    "def standardize(X):\n",
    "    X_std = np.zeros(X.shape)\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "\n",
    "    # 做除法运算时请永远记住分母不能等于0的情形\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0) \n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "\n",
    "    return X_std\n",
    "\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "def train_test_split(X, y, test_size=0.2, shuffle=True, seed=None):\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "\n",
    "    n_train_samples = int(X.shape[0] * (1-test_size))\n",
    "    x_train, x_test = X[:n_train_samples], X[n_train_samples:]\n",
    "    y_train, y_test = y[:n_train_samples], y[n_train_samples:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "    return np.sum(y == y_pred)/len(y)\n",
    "\n",
    "\n",
    "class KNN():\n",
    "    \"\"\" K近邻分类算法.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k: int\n",
    "        最近邻个数.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    # 计算一个样本与训练集中所有样本的欧氏距离的平方\n",
    "    def euclidean_distance(self, one_sample, X_train):\n",
    "        one_sample = one_sample.reshape(1, -1)\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        distances = np.power(np.tile(one_sample, (X_train.shape[0], 1)) - X_train, 2).sum(axis=1)\n",
    "        return distances\n",
    "    \n",
    "    # 获取k个近邻的类别标签\n",
    "    def get_k_neighbor_labels(self, distances, y_train, k):\n",
    "        k_neighbor_labels = []\n",
    "        for distance in np.sort(distances)[:k]:\n",
    "\n",
    "            label = y_train[distances==distance]\n",
    "            k_neighbor_labels.append(label)\n",
    "\n",
    "        return np.array(k_neighbor_labels).reshape(-1, )\n",
    "    \n",
    "    # 进行标签统计，得票最多的标签就是该测试样本的预测标签\n",
    "    def vote(self, one_sample, X_train, y_train, k):\n",
    "        distances = self.euclidean_distance(one_sample, X_train)\n",
    "        #print(distances.shape)\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "        k_neighbor_labels = self.get_k_neighbor_labels(distances, y_train, k)\n",
    "        #print(k_neighbor_labels.shape)\n",
    "        find_label, find_count = 0, 0\n",
    "        for label, count in Counter(k_neighbor_labels).items():\n",
    "            if count > find_count:\n",
    "                find_count = count\n",
    "                find_label = label\n",
    "        return find_label\n",
    "    \n",
    "    # 对测试集进行预测\n",
    "    def predict(self, X_test, X_train, y_train):\n",
    "        y_pred = []\n",
    "        for sample in X_test:\n",
    "            label = self.vote(sample, X_train, y_train, self.k)\n",
    "            y_pred.append(label)\n",
    "        #print(y_pred)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = make_classification(n_samples=200, n_features=4, n_informative=2, \n",
    "                               n_redundant=2, n_repeated=0, n_classes=2)\n",
    "    X, y = data[0], data[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True)\n",
    "    clf = KNN(k=5)\n",
    "    y_pred = clf.predict(X_test, X_train, y_train)\n",
    "    \n",
    "    accu = accuracy(y_test, y_pred)\n",
    "    print (\"Accuracy:\", accu)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def shuffle_data(X, y, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "\n",
    "# 正规化数据集 X\n",
    "def normalize(X, axis=-1, p=2):\n",
    "    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))\n",
    "    lp_norm[lp_norm == 0] = 1\n",
    "    return X / np.expand_dims(lp_norm, axis)\n",
    "\n",
    "\n",
    "# 标准化数据集 X\n",
    "def standardize(X):\n",
    "    X_std = np.zeros(X.shape)\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "\n",
    "    # 做除法运算时请永远记住分母不能等于0的情形\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0) \n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "\n",
    "    return X_std\n",
    "\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "def train_test_split(X, y, test_size=0.2, shuffle=True, seed=None):\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "\n",
    "    n_train_samples = int(X.shape[0] * (1-test_size))\n",
    "    x_train, x_test = X[:n_train_samples], X[n_train_samples:]\n",
    "    y_train, y_test = y[:n_train_samples], y[n_train_samples:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "    return np.sum(y == y_pred)/len(y)\n",
    "\n",
    "\n",
    "class KNN():\n",
    "    \"\"\" K近邻分类算法.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k: int\n",
    "        最近邻个数.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    # 计算一个样本与训练集中所有样本的欧氏距离\n",
    "    def euclidean_distance(slef, one_sample, X_train):\n",
    "        one_sample = one_sample.reshape(1, -1)\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        distances = np.power(np.tile(one_sample, (X_train.shape[0], 1)) - X_train, \n",
    "                             2).sum(axis=1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "    def get_k_neighbor(self, distances, k):\n",
    "        idx = np.argsort(distances)<k\n",
    "        return idx\n",
    "\n",
    "    def _vote(self, one_sample, X_train, y_train, k):\n",
    "        distances = self.euclidean_distance(one_sample, X_train)\n",
    "        #print(distances)\n",
    "        idx = self.get_k_neighbor(distances, k)\n",
    "        #print(distances[idx])\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "        labels = [label for label in y_train[idx][:, 0]]\n",
    "        dict_labels = Counter(labels)\n",
    "        #print(dict_labels)\n",
    "\n",
    "        find_label, find_count = 0, 0\n",
    "        for label, count in dict_labels.items():\n",
    "            if count>find_count:\n",
    "                find_count = count\n",
    "                find_label = label\n",
    "        return find_label\n",
    "\n",
    "\n",
    "    def predict(self, X_test, X_train, y_train):\n",
    "        y_pred = []\n",
    "        for sample in X_test:\n",
    "            label = self._vote(sample, X_train, y_train, self.k)\n",
    "            y_pred.append(label)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = datasets.load_iris()\n",
    "    X = normalize(data.data)\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True)\n",
    "\n",
    "    clf = KNN(k=5)\n",
    "    y_pred = clf.predict(X_test, X_train, y_train)\n",
    "    \n",
    "    accu = accuracy(y_test, y_pred)\n",
    "\n",
    "    print (\"Accuracy:\", accu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
